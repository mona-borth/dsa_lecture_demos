{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "------------------\n",
        "```markdown\n",
        "# Copyright © 2024 Meysam Goodarzi\n",
        "This notebook is licensed under CC BY-NC 4.0 with the following amandments:\n",
        "- Individuals may use, share, and adapt this material for non-commercial purposes with attribution.\n",
        "- Institutions/Companies must obtain written consent to use this material, except for nonprofits.\n",
        "- Commercial use is prohibited without permission.  \n",
        "Contact: analytica@meysam-goodarzi.com\n",
        "```\n",
        "------------------------------\n",
        "❗❗❗ **IMPORTANT**❗❗❗ **Create a copy of this notebook**\n",
        "\n",
        "In order to work with this Google Colab you need to create a copy of it. Please **DO NOT** provide your answers here. Instead, work on the copy version. To make a copy:\n",
        "\n",
        "**Click on: File -> save a copy in drive**\n",
        "\n",
        "Have you successfully created the copy? if yes, there must be a new tab opened in your browser. Now move to the copy and start from there!\n",
        "\n",
        "----------------------------------------------\n"
      ],
      "metadata": {
        "id": "B2GJW6FIByoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sb\n",
        "from scipy.stats import weibull_min, lognorm\n",
        "\n",
        "!pip install quantecon\n",
        "import quantecon as qe"
      ],
      "metadata": {
        "id": "Y8adHXxV5h0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15932748-3fc8-4eca-cbc9-ea1ce5a0b53d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting quantecon\n",
            "  Downloading quantecon-0.8.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: numba>=0.49.0 in /usr/local/lib/python3.11/dist-packages (from quantecon) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from quantecon) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from quantecon) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from quantecon) (1.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from quantecon) (1.13.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.49.0->quantecon) (0.43.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->quantecon) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->quantecon) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->quantecon) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->quantecon) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->quantecon) (1.3.0)\n",
            "Downloading quantecon-0.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.7/322.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: quantecon\n",
            "Successfully installed quantecon-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Income Distribution\n",
        "There are multiple distribution that are normally chosen to represent the real-worl income distributions. Those are:\n",
        "* Weibull\n",
        "* Log-normal\n",
        "* Exponential"
      ],
      "metadata": {
        "id": "imjaF8fc5GIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weibull Distribution\n",
        "The Weibull distribution is a continuous probability distribution commonly used in reliability analysis, survival studies, and income distributions. It is highly flexible and can model different types of data by adjusting its shape parameter.The probability density function (PDF) of the Weibull distribution is:\n",
        "$$\n",
        "f(x; c, \\lambda) = \\frac{c}{\\lambda} \\left(\\frac{x}{\\lambda}\\right)^{c-1} e^{-(x/\\lambda)^c}, \\quad x \\geq 0\n",
        "$$\n",
        "where:  \n",
        "- $ x $ is the random variable (income, lifespan, etc.).\n",
        "-$ c $ (shape parameter) controls the shape of the distribution:\n",
        "  - If $ c < 1 $,  the distribution is decreasing (high failure rates at the beginning, e.g., early-life failures).\n",
        "  - If $ c = 1 $ it reduces to an exponential distribution (constant failure rate).\n",
        "  - If $ c > 1 $ it is bell-shaped, modeling a life cycle with increasing failure rates over time.\n",
        "- $ \\lambda $ (scale parameter) stretches or shrinks the distribution."
      ],
      "metadata": {
        "id": "wpVaU4eh5eqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weibull distribution parameters\n",
        "c, loc, scale, population = 1.8, 0, 43.8, 10000  # Shape, location, scale, and population size\n",
        "\n",
        "# Generate income using Weibull distribution\n",
        "income = weibull_min.rvs(c=c, loc=loc, scale=scale, size=population)\n",
        "\n",
        "# Compute the top decile income share (income share of the top 100 earners)\n",
        "top_dec = np.sum(np.sort(income)[-100:]) / np.sum(income)\n",
        "print(f\"Top decile income share = {top_dec:.2f}\")\n",
        "\n",
        "# Compute median and mean income\n",
        "median = np.median(income)\n",
        "mean = np.mean(income)\n",
        "\n",
        "# Define histogram bin edges (income ranges)\n",
        "bins = [0] + list(range(1, 150, 1)) + [175, 200, 300, 500]\n",
        "\n",
        "# Compute histogram of income distribution\n",
        "hist, bin_edges = np.histogram(income, bins=bins, density=False)\n",
        "y = [0, max(hist)]  # Y-axis range for reference lines\n",
        "\n",
        "# Compute Lorenz curve and Gini coefficient\n",
        "cdf, xcdf = qe.lorenz_curve(income)\n",
        "gini = qe.gini_coefficient(income)\n",
        "print(f\"Gini Coefficient = {gini:.2f}\")\n",
        "\n",
        "# --------- Plot 1: Income Distribution Histogram ---------\n",
        "fig, ax = plt.subplots(figsize=(7, 3))\n",
        "\n",
        "# Bar chart of income distribution\n",
        "ax.bar(bins[:-1], hist, color=\"black\")\n",
        "\n",
        "# Vertical lines for mean and median income\n",
        "ax.plot([median, median], y, color=\"red\", label=f\"Median income = {median:.2f}\")\n",
        "ax.plot([mean, mean], y, color=\"blue\", label=f\"Mean income = {mean:.2f}\")\n",
        "\n",
        "# Labels and title\n",
        "ax.set_title(\"Income Distribution (Weibull dist.)\")\n",
        "ax.set_xlabel(\"Income (k€)\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend()\n",
        "\n",
        "# --------- Plot 2: Lorenz Curve for Income Inequality ---------\n",
        "fig, ax = plt.subplots(figsize=(7, 3))\n",
        "\n",
        "# Lorenz curve: Shows cumulative income distribution\n",
        "ax.plot(cdf, xcdf, color=\"black\", label=\"Cumulative income\")\n",
        "\n",
        "# Perfect equality line (45-degree line)\n",
        "ax.plot(cdf, cdf, color=\"blue\", linestyle=\"--\", label=\"Perfect equality\")\n",
        "\n",
        "# Labels and legend\n",
        "ax.set_xlabel(\"Population portion\")\n",
        "ax.set_ylabel(\"Income portion\")\n",
        "ax.legend()\n",
        "\n",
        "# Show both plots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wTE2wOj_5n1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log-normal Distribution\n",
        "The log-normal distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed. It is widely used in economics, finance, and reliability analysis to model variables that are positively skewed, such as income distributions. The probability density function of a log-normal distribution is:\n",
        "\n",
        "$$\n",
        "f(x; \\mu, \\sigma) = \\frac{1}{x \\sigma \\sqrt{2\\pi}} e^{-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}}, \\quad x > 0\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- $ x $ is the random variable (e.g., income).  \n",
        "- $ \\mu $ location parameter is the mean of the natural logarithm of $ x $  \n",
        "- $ \\sigma $ scale parameter is the standard deviation of the natural logarithm of $ x $  "
      ],
      "metadata": {
        "id": "F29WtXZq5vTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters for the log-normal distribution\n",
        "# Standard deviation of log(X)\n",
        "shape_param = 1.0\n",
        "# Location parameter (shift)\n",
        "loc_param = 10\n",
        "# Scale parameter (median of exp(X))\n",
        "scale_param = 1.5\n",
        "population_size = 1000\n",
        "\n",
        "# Generate income data from a log-normal distribution\n",
        "income = lognorm.rvs(s=shape_param, loc=loc_param, scale=scale_param, size=population_size)\n",
        "\n",
        "# Calculate top decile income share (percentage of total income held by the top 10%)\n",
        "top_decile_income = np.sum(np.sort(income)[-100:]) / np.sum(income)\n",
        "print(f\"Top Decile Income Share: {top_decile_income:.2f}\")\n",
        "\n",
        "# Compute key statistics\n",
        "median_income = np.median(income)\n",
        "mean_income = np.mean(income)\n",
        "\n",
        "# Create histogram bins automatically\n",
        "hist, bin_edges = np.histogram(income, bins=\"auto\", density=False)\n",
        "y_range = [0, max(hist)]  # Used for vertical lines indicating median/mean\n",
        "\n",
        "# Compute Lorenz curve and Gini coefficient\n",
        "cdf, xcdf = qe.lorenz_curve(income)\n",
        "gini_coeff = qe.gini_coefficient(income)\n",
        "print(f\"Gini Coefficient: {gini_coeff:.2f}\")\n",
        "\n",
        "# Plot histogram of income distribution\n",
        "fig, ax = plt.subplots(figsize=(7, 3))\n",
        "ax.bar(bin_edges[:-1], hist, color=\"black\", alpha=0.7, label=\"Income Distribution\")\n",
        "ax.axvline(median_income, color=\"red\", linestyle=\"--\", label=f\"Median Income = {median_income:.2f}\")\n",
        "ax.axvline(mean_income, color=\"blue\", linestyle=\"--\", label=f\"Mean Income = {mean_income:.2f}\")\n",
        "\n",
        "ax.set_title(\"Income Distribution (Log-Normal)\")\n",
        "ax.set_xlabel(\"Income (€)\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.legend()\n",
        "\n",
        "# Plot Lorenz curve for income inequality\n",
        "fig, ax = plt.subplots(figsize=(7, 3))\n",
        "ax.plot(cdf, xcdf, color=\"black\", label=\"Cumulative Income\")\n",
        "ax.plot(cdf, cdf, color=\"blue\", linestyle=\"--\", label=\"Perfect Equality Line\")\n",
        "\n",
        "ax.set_title(\"Lorenz Curve\")\n",
        "ax.set_xlabel(\"Population Portion\")\n",
        "ax.set_ylabel(\"Income Portion\")\n",
        "ax.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "klTLPiwf6DsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projected Gradient Descent (PGD)\n",
        "\n",
        "Given a convex function $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ and a convex set $\\mathcal{C} \\subseteq \\mathbb{R}^n$, we would like to solve:\n",
        "$$\n",
        "\\min_{x \\in \\mathcal{C}} f(x)\n",
        "$$\n",
        "where $\\mathcal{C}$ represents constraints and can be written in the form of:\n",
        "- $f_i(x) \\leq 0 \\forall i = 0, \\cdots, N$\n",
        "- $h_i(x) = 0 \\forall i = 0, \\cdots, M$\n",
        "\n",
        "The Projected Gradient Descent (PGD) can be implemented to solve above problem by using the following steps:\n",
        "1. Initial $x_0 \\in \\mathcal{C}$, step size $\\alpha > 0$, max iterations $T$\n",
        "1. Compute gradient: $g_k = \\nabla f(x_k)$\n",
        "1. Take gradient step: $x_{k+1/2} = x_k - \\alpha g_k$\n",
        "1. Project onto $\\mathcal{C}$: $x_{k+1} = \\mathcal{P}_\\mathcal{C}(x_{k+1/2})$ if constaints are violated\n",
        "1. Repeat from step 2 until convergence\n",
        "\n"
      ],
      "metadata": {
        "id": "J-o6nrT2wVoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example\n",
        "Let us use the above algorithm to solve the following constrained optimization problem:\n",
        "$$\n",
        "\\min_{x, y} f(x, y) = (x-1)^2 + (y-2)^2 + xy\n",
        "$$\n",
        "subject to:\n",
        "$$2x+3y \\geq 10 $$"
      ],
      "metadata": {
        "id": "ln0AqbCEz4iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x: float, y: float) -> float:\n",
        "    \"\"\"Computes the objective function value.\n",
        "\n",
        "    Args:\n",
        "        x (float): The x-coordinate.\n",
        "        y (float): The y-coordinate.\n",
        "\n",
        "    Returns:\n",
        "        float: The computed function value.\n",
        "    \"\"\"\n",
        "    return (x - 1)**2 + (y - 2)**2 + x * y\n",
        "\n",
        "def gradient(x: float, y: float) -> np.ndarray:\n",
        "    \"\"\"Computes the gradient vector of the function f(x, y).\n",
        "\n",
        "    Args:\n",
        "        x (float): The x-coordinate.\n",
        "        y (float): The y-coordinate.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The gradient vector [df/dx, df/dy].\n",
        "    \"\"\"\n",
        "    return np.array([2 * (x - 1) + y, 2 * (y - 2) + x])\n",
        "\n",
        "# Constraint: 2x + 3y >= 10 (a=2, b=3, c=10)\n",
        "a, b, c = 2, 3, 10\n",
        "\n",
        "def project_onto_constraint(x: float, y: float) -> tuple[float, float]:\n",
        "    \"\"\"Projects the point (x, y) onto the constraint hyperplane 2x + 3y = 10.\n",
        "\n",
        "    Args:\n",
        "        x (float): The x-coordinate.\n",
        "        y (float): The y-coordinate.\n",
        "\n",
        "    Returns:\n",
        "        tuple[float, float]: The projected coordinates (x_proj, y_proj).\n",
        "    \"\"\"\n",
        "    denom = a**2 + b**2\n",
        "    x_proj = x - a * (a*x + b*y - c) / denom\n",
        "    y_proj = y - b * (a*x + b*y - c) / denom\n",
        "    return x_proj, y_proj\n",
        "\n",
        "def projected_gradient_descent(max_iters: int, tol: float, alpha: float) -> tuple[float, float, list[tuple[float, float]]]:\n",
        "    \"\"\"Performs projected gradient descent with constraints.\n",
        "\n",
        "    Args:\n",
        "        max_iters (int): Maximum number of iterations.\n",
        "        tol (float): Convergence tolerance.\n",
        "        alpha (float): Step size for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "        tuple[float, float, list[tuple[float, float]]]:\n",
        "            - The optimal x-coordinate.\n",
        "            - The optimal y-coordinate.\n",
        "            - A list of all (x, y) points visited during optimization.\n",
        "    \"\"\"\n",
        "    # Initialize with a feasible point\n",
        "    x, y = 4.0, 5.0\n",
        "    # Store all iterations\n",
        "    history = [(x, y)]\n",
        "    print(f\"Iter 0: x = {x:.4f}, y = {y:.4f}, f(x,y) = {f(x, y):.4f}, Constraint: {2*x + 3*y >= 10}\")\n",
        "\n",
        "    for k in range(1, max_iters + 1):\n",
        "        grad = gradient(x, y)\n",
        "        x_temp = x - alpha * grad[0]\n",
        "        y_temp = y - alpha * grad[1]\n",
        "\n",
        "        if 2*x_temp + 3*y_temp < 10:\n",
        "            x_new, y_new = project_onto_constraint(x_temp, y_temp)\n",
        "        else:\n",
        "            x_new, y_new = x_temp, y_temp\n",
        "\n",
        "        history.append((x_new, y_new))\n",
        "\n",
        "        if np.linalg.norm([x_new - x, y_new - y]) < tol:\n",
        "            x, y = x_new, y_new\n",
        "            print(f\"Iter {k}: x = {x:.4f}, y = {y:.4f}, f(x,y) = {f(x, y):.4f}, Constraint: {2*x + 3*y >= 10}\")\n",
        "            print(f\"Converged at iteration {k}\")\n",
        "            break\n",
        "\n",
        "        x, y = x_new, y_new\n",
        "        print(f\"Iter {k}: x = {x:.4f}, y = {y:.4f}, f(x,y) = {f(x, y):.4f}, Constraint: {2*x + 3*y >= 10}\")\n",
        "\n",
        "    return x, y, history\n",
        "\n",
        "# Run the algorithm\n",
        "x_opt, y_opt, history = projected_gradient_descent(max_iters=1000, tol=1e-2, alpha=0.1)\n",
        "print(f\"\\nOptimal solution: x* = {x_opt:.4f}, y* = {y_opt:.4f}\")\n",
        "print(f\"Objective value: {f(x_opt, y_opt):.4f}\")\n",
        "print(f\"Constraint satisfied: {2*x_opt + 3*y_opt >= 10}\")\n",
        "print(f\"Constraint Value: {2*x_opt + 3*y_opt}\")"
      ],
      "metadata": {
        "id": "mFaSd_Raxfn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now plot the results."
      ],
      "metadata": {
        "id": "pCb78SfE4oWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create contour plot\n",
        "limit_low = 1\n",
        "limit_up = 5\n",
        "x_vals = np.linspace(-limit_low, limit_up, 100)\n",
        "y_vals = np.linspace(-limit_low, limit_up, 100)\n",
        "X, Y = np.meshgrid(x_vals, y_vals)\n",
        "Z = f(X, Y)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot contours\n",
        "contours = plt.contour(X, Y, Z, levels=20, cmap='viridis')\n",
        "plt.colorbar(contours, label='Objective Value')\n",
        "\n",
        "# Plot constraint boundary (2x + 3y = 10)\n",
        "constraint_x = np.linspace(-limit_low, limit_up, 100)\n",
        "constraint_y = (10 - 2 * constraint_x) / 3\n",
        "plt.plot(constraint_x, constraint_y, 'r-', label='Constraint: 2x + 3y = 10')\n",
        "\n",
        "# Shade feasible region (2x + 3y >= 10)\n",
        "plt.fill_between(constraint_x, constraint_y, limit_up, color='red', alpha=0.1, label='Feasible Region')\n",
        "\n",
        "# Plot PGD iterations\n",
        "x_hist, y_hist = zip(*history)\n",
        "plt.scatter(x_hist, y_hist, c='white', s=30, edgecolors='black', label='PGD Iterations')\n",
        "plt.plot(x_hist, y_hist, 'k--', lw=1, alpha=0.5)\n",
        "\n",
        "# Mark initial and optimal points\n",
        "plt.scatter(x_hist[0], y_hist[0], c='blue', s=100, label='Initial Point', marker='s')\n",
        "plt.scatter(x_opt, y_opt, c='red', s=100, label='Optimal Point', marker='*')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Projected Gradient Descent Optimization')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xlim(-limit_low, limit_up)\n",
        "plt.ylim(-limit_low, limit_up)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m65piDVV1BeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1\n",
        "\n",
        "Let us use the above algorithm to solve the following constrained optimization problem:\n",
        "$$\n",
        "\\min_{x, y} f(x, y) = (x-1)^2 + (y-2)^2 + xy\n",
        "$$\n",
        "subject to:\n",
        "$$ (x-5)^2 + (y-4)^2 \\leq 8 $$"
      ],
      "metadata": {
        "id": "Rf6sHRcydFjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x: float, y: float) -> float:\n",
        "    \"\"\"Computes the objective function value.\n",
        "\n",
        "    Args:\n",
        "        x (float): The x-coordinate.\n",
        "        y (float): The y-coordinate.\n",
        "\n",
        "    Returns:\n",
        "        float: The computed function value.\n",
        "    \"\"\"\n",
        "    return (x - 1)**2 + (y - 2)**2 + x * y\n",
        "\n",
        "def grad_x(x: float, y: float) -> float:\n",
        "    \"\"\"Computes the partial derivative of f with respect to x.\n",
        "\n",
        "    Args:\n",
        "        x (float): The x-coordinate.\n",
        "        y (float): The y-coordinate.\n",
        "\n",
        "    Returns:\n",
        "        float: The gradient component df/dx.\n",
        "    \"\"\"\n",
        "    return 2 * (x - 1) + y\n",
        "\n",
        "def grad_y(x: float, y: float) -> float:\n",
        "    \"\"\"Computes the partial derivative of f with respect to y.\n",
        "\n",
        "    Args:\n",
        "        x (float): The x-coordinate.\n",
        "        y (float): The y-coordinate.\n",
        "\n",
        "    Returns:\n",
        "        float: The gradient component df/dy.\n",
        "    \"\"\"\n",
        "    return 2 * (y - 2) + x\n",
        "\n",
        "radius: float = 8\n",
        "\n",
        "def project_onto_circle(x: float, y: float, radius: float) -> tuple[float, float]:\n",
        "    \"\"\"Projects (x, y) onto the circle (x-5)^2 + (y-4)^2 <= 3.\n",
        "\n",
        "    Args:\n",
        "        x (float): The x-coordinate.\n",
        "        y (float): The y-coordinate.\n",
        "        radius (float): The radius of the constraint circle.\n",
        "\n",
        "    Returns:\n",
        "        tuple[float, float]: The projected coordinates (x_proj, y_proj).\n",
        "    \"\"\"\n",
        "    center_x, center_y = 5, 4\n",
        "    radius = np.sqrt(radius)\n",
        "\n",
        "    # Compute distance from center\n",
        "    dx = x - center_x\n",
        "    dy = y - center_y\n",
        "    distance = np.sqrt(dx**2 + dy**2)\n",
        "\n",
        "    # Project onto boundary\n",
        "    x_proj = # Your code\n",
        "    y_proj = # Your code\n",
        "    return x_proj, y_proj\n",
        "\n",
        "def pgd_circle(max_iters: int, alpha: float, tol: float) -> tuple[float, float, list[tuple[float, float]]]:\n",
        "    \"\"\"Performs projected gradient descent constrained within a circle.\n",
        "\n",
        "    Args:\n",
        "        max_iters (int): Maximum number of iterations.\n",
        "        alpha (float): Step size for gradient descent.\n",
        "        tol (float): Convergence tolerance.\n",
        "\n",
        "    Returns:\n",
        "        tuple[float, float, list[tuple[float, float]]]:\n",
        "            - The optimal x-coordinate.\n",
        "            - The optimal y-coordinate.\n",
        "            - A list of all (x, y) points visited during optimization.\n",
        "    \"\"\"\n",
        "    x, y = 5.0, 3.0  # Initialize with an infeasible point outside the circle\n",
        "    history = [(x, y)]\n",
        "\n",
        "    for k in range(max_iters):\n",
        "        # Gradient step\n",
        "        x_temp = x - alpha * grad_x(x, y)\n",
        "        y_temp = # Your code\n",
        "\n",
        "        # Projection step\n",
        "        if (x_temp - 5)**2 + (y_temp - 4)**2 <= radius:\n",
        "            x_new, y_new = x_temp, y_temp\n",
        "        else:\n",
        "            x_new, y_new = project_onto_circle(x_temp, y_temp, radius)\n",
        "\n",
        "        # Check convergence\n",
        "        if np.linalg.norm([x_new - x, y_new - y]) < tol:\n",
        "            x, y = x_new, y_new\n",
        "            history.append((x, y))\n",
        "            print(f\"Iter {k}: x = {x:.4f}, y = {y:.4f}, f(x,y) = {f(x, y):.4f}, Constraint: {(x-5)**2 + (y-4)**2 <= radius}\")\n",
        "            print(f\"Converged at iteration {k}\")\n",
        "            break\n",
        "\n",
        "        x, y = x_new, y_new\n",
        "        history.append((x, y))\n",
        "        print(f\"Iter {k}: x = {x:.4f}, y = {y:.4f}, f(x,y) = {f(x, y):.4f}, Constraint: {(x-5)**2 + (y-4)**2 <= radius}\")\n",
        "\n",
        "    return x, y, history\n",
        "\n",
        "# Run PGD\n",
        "x_opt, y_opt, history = pgd_circle(max_iters=1000, alpha=0.1, tol=1e-3)\n",
        "print(f\"Optimal solution: x* = {x_opt:.4f}, y* = {y_opt:.4f}\")\n",
        "print(f\"Objective value: {f(x_opt, y_opt):.4f}\")\n",
        "print(f\"Constraint check: {(x_opt-5)**2 + (y_opt-4)**2 <= 3}\")\n",
        "print(f\"Constraint Value: {(x_opt-5)**2 + (y_opt-4)**2}\")\n"
      ],
      "metadata": {
        "id": "jarxRcrgfLDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "limit_low = -1\n",
        "limit_up = 6\n",
        "radius = 8\n",
        "\n",
        "# Plot the circle constraint\n",
        "theta = np.linspace(0, 2*np.pi, 100)\n",
        "circle_x = 5 + np.sqrt(radius) * np.cos(theta)\n",
        "circle_y = 4 + np.sqrt(radius) * np.sin(theta)\n",
        "\n",
        "# Plot objective contours\n",
        "x_vals = np.linspace(limit_low, limit_up, 100)\n",
        "y_vals = np.linspace(limit_low, limit_up, 100)\n",
        "X, Y = np.meshgrid(x_vals, y_vals)\n",
        "Z = f(X, Y)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.contour(X, Y, Z, levels=20, cmap='viridis')\n",
        "plt.colorbar(label='Objective Value')\n",
        "plt.plot(circle_x, circle_y, 'r-', label='Constraint Boundary')\n",
        "plt.fill(circle_x, circle_y, color='red', alpha=0.1, label='Feasible Region')\n",
        "\n",
        "# Plot PGD path\n",
        "x_hist, y_hist = zip(*history)\n",
        "plt.scatter(x_hist, y_hist, c='white', s=30, edgecolors='black')\n",
        "plt.plot(x_hist, y_hist, 'k--', lw=1, alpha=0.5, label='PGD Path')\n",
        "\n",
        "# Mark points\n",
        "plt.scatter(x_hist[0], y_hist[0], c='blue', s=100, label='Initial Point', marker='s')\n",
        "plt.scatter(x_opt, y_opt, c='red', s=100, label='Optimal Point', marker='*')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('PGD with Circle Constraint')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xlim(limit_low, limit_up)\n",
        "plt.ylim(limit_low, limit_up)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Uy--bdGdfQna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternating Optimization\n",
        "To solve a Biconvex problem:\n",
        "1. Partition $\\mathbf{x}=(X_1,X_2,\\cdots,X_i)$ into sets of non-overlapping variables\n",
        "1. Compute $$X_i^{(r+1)} = \\textit{argmin/max}\\ f(X_1^{(r+1)}, X_2^{(r+1)}, \\cdots, X_i, \\cdots, X_t^{(r)}),$$ with $i=1, \\cdots, t$ where $r$ is the index of the current iteration.\n",
        "1. If $||x^{(r+1)}-x^{(r)}|| \\leq \\epsilon $ or $r\\geq L$, then quit; otherwise, set $r=r+1$ and go to step 2"
      ],
      "metadata": {
        "id": "9HRzeMw4s3O1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example\n",
        "Let us use the above algorithm to solve the following constrained optimization problem:\n",
        "$$\n",
        "\\min_{x, y} f(x, y) = (x-1)^2 + (y-2)^2 + xy\n",
        "$$\n",
        "subject to:\n",
        "$$x \\geq 1$$\n",
        "$$y \\geq 3$$"
      ],
      "metadata": {
        "id": "MOHakq6HJGWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x: float, y: float) -> float:\n",
        "    \"\"\"Computes the objective function value.\n",
        "\n",
        "    Args:\n",
        "        x (float): The x-coordinate.\n",
        "        y (float): The y-coordinate.\n",
        "\n",
        "    Returns:\n",
        "        float: The computed function value.\n",
        "    \"\"\"\n",
        "    return (x - 1)**2 + (y - 2)**2 + x * y\n",
        "\n",
        "def grad_x(x: float, y: float) -> float:\n",
        "    \"\"\"Computes the partial derivative of f with respect to x.\n",
        "\n",
        "    Args:\n",
        "        x (float): The x-coordinate.\n",
        "        y (float): The y-coordinate.\n",
        "\n",
        "    Returns:\n",
        "        float: The gradient component df/dx.\n",
        "    \"\"\"\n",
        "    return 2 * (x - 1) + y\n",
        "\n",
        "def grad_y(x: float, y: float) -> float:\n",
        "    \"\"\"Computes the partial derivative of f with respect to y.\n",
        "\n",
        "    Args:\n",
        "        x (float): The x-coordinate.\n",
        "        y (float): The y-coordinate.\n",
        "\n",
        "    Returns:\n",
        "        float: The gradient component df/dy.\n",
        "    \"\"\"\n",
        "    return 2 * (y - 2) + x\n",
        "\n",
        "def ao_pgd(max_iters: int, tol: float, alpha_x: float, alpha_y: float) -> tuple[float, float, list[tuple[float, float]]]:\n",
        "    \"\"\"Performs alternating optimization projected gradient descent.\n",
        "\n",
        "    Args:\n",
        "        max_iters (int): Maximum number of iterations.\n",
        "        tol (float): Convergence tolerance.\n",
        "        alpha_x (float): Step size for x update.\n",
        "        alpha_y (float): Step size for y update.\n",
        "\n",
        "    Returns:\n",
        "        tuple[float, float, list[tuple[float, float]]]:\n",
        "            - The optimal x-coordinate.\n",
        "            - The optimal y-coordinate.\n",
        "            - A list of all (x, y) points visited during optimization.\n",
        "    \"\"\"\n",
        "    # Initialize with a feasible point (x >= 1 and y >= 3)\n",
        "    x, y = 5.0, 4.0\n",
        "    # Store all iterations\n",
        "    history = [(x, y)]\n",
        "    print(f\"Iter 0: x = {x:.4f}, y = {y:.4f}, f(x,y) = {f(x, y):.4f}, Constraint: {(x >= 1) and (y >= 3)}\")\n",
        "\n",
        "    for k in range(1, max_iters + 1):\n",
        "        # Step 1: Fix y, update x\n",
        "        x_temp = x - alpha_x * grad_x(x, y)\n",
        "\n",
        "        # Project onto x >= 1\n",
        "        x_new = max(x_temp, 1.0)\n",
        "\n",
        "        # Step 2: Fix x_new, update y\n",
        "        y_temp = y - alpha_y * grad_y(x_new, y)\n",
        "\n",
        "        # Project onto y >= 3\n",
        "        y_new = max(y_temp, 3.0)\n",
        "\n",
        "        # Store the new point\n",
        "        history.append((x_new, y_new))\n",
        "\n",
        "        # Check convergence\n",
        "        if np.linalg.norm([x_new - x, y_new - y]) < tol:\n",
        "            x, y = x_new, y_new\n",
        "            print(f\"Iter {k}: x = {x:.4f}, y = {y:.4f}, f(x,y) = {f(x, y):.4f}, Constraint: {(x >= 1) and (y >= 3)}\")\n",
        "            print(f\"Converged at iteration {k}\")\n",
        "            break\n",
        "\n",
        "        x, y = x_new, y_new\n",
        "        print(f\"Iter {k}: x = {x:.4f}, y = {y:.4f}, f(x,y) = {f(x, y):.4f}, Constraint: {(x >= 1) and (y >= 3)}\")\n",
        "\n",
        "    return x, y, history\n",
        "\n",
        "# Run the algorithm\n",
        "x_opt, y_opt, history = ao_pgd(max_iters=1000, tol=1e-2, alpha_x=0.1, alpha_y=0.1)\n",
        "print(f\"\\nOptimal solution: x* = {x_opt:.4f}, y* = {y_opt:.4f}, Constraint satisfied: {(x_opt >= 1) and (y_opt >= 3)}\")\n"
      ],
      "metadata": {
        "id": "wx4BVaoVZ9Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now plot the results."
      ],
      "metadata": {
        "id": "CZbEyLTw4r1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create contour plot\n",
        "limit_low = -1\n",
        "limit_up = 6\n",
        "x_vals = np.linspace(limit_low, limit_up, 100)\n",
        "y_vals = np.linspace(limit_low, limit_up, 100)\n",
        "X, Y = np.meshgrid(x_vals, y_vals)\n",
        "Z = f(X, Y)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot contours\n",
        "contours = plt.contour(X, Y, Z, levels=20, cmap='viridis')\n",
        "plt.colorbar(contours, label='Objective Value')\n",
        "\n",
        "# Plot constraint boundary\n",
        "constraint_x = 1\n",
        "constraint_y = 3\n",
        "# Plot horizontal and vertical lines\n",
        "plt.axhline(y=constraint_y, color='r', linestyle='--', label=\"y = 0\")  # Horizontal line at y=0\n",
        "plt.axvline(x=constraint_x, color='b', linestyle='-.', label=\"x = 0\")  # Vertical line at x=0\n",
        "\n",
        "# Shade feasible region (x > 1 and y > 3)\n",
        "x_shade = [constraint_x, limit_up, limit_up, constraint_x]  # x-coordinates of shaded area\n",
        "y_shade = [constraint_y, constraint_y, limit_up, limit_up]  # y-coordinates of shaded area\n",
        "plt.fill(x_shade, y_shade, color='red', alpha=0.3, label='Feasible Region')\n",
        "\n",
        "# Plot PGD iterations\n",
        "x_hist, y_hist = zip(*history)\n",
        "plt.scatter(x_hist, y_hist, c='white', s=30, edgecolors='black', label='PGD Iterations')\n",
        "plt.plot(x_hist, y_hist, 'k--', lw=1, alpha=0.5)\n",
        "\n",
        "# Mark initial and optimal points\n",
        "plt.scatter(x_hist[0], y_hist[0], c='blue', s=100, label='Initial Point', marker='s')\n",
        "plt.scatter(x_opt, y_opt, c='red', s=100, label='Optimal Point', marker='*')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Projected Gradient Descent Optimization')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xlim(limit_low, limit_up)\n",
        "plt.ylim(limit_low, limit_up)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4AHm7bJp4nY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "\n",
        "Let us use the above algorithm to solve the following constrained optimization problem:\n",
        "$$\n",
        "\\min_{x, y} f(x, y) = (x-1)^2 + (y-2)^2 + xy\n",
        "$$\n",
        "subject to:\n",
        "$$x^2 \\geq 2$$\n",
        "$$y \\geq 3$$"
      ],
      "metadata": {
        "id": "4wwfWef2JCyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x: float, y: float) -> float:\n",
        "    \"\"\"Computes the objective function value.\n",
        "\n",
        "    Args:\n",
        "        x (float): The x-coordinate.\n",
        "        y (float): The y-coordinate.\n",
        "\n",
        "    Returns:\n",
        "        float: The computed function value.\n",
        "    \"\"\"\n",
        "    return (x - 1)**2 + (y - 2)**2 + x * y\n",
        "\n",
        "def grad_x(x: float, y: float) -> float:\n",
        "    \"\"\"Computes the partial derivative of f with respect to x.\n",
        "\n",
        "    Args:\n",
        "        x (float): The x-coordinate.\n",
        "        y (float): The y-coordinate.\n",
        "\n",
        "    Returns:\n",
        "        float: The gradient component df/dx.\n",
        "    \"\"\"\n",
        "    return 2 * (x - 1) + y\n",
        "\n",
        "def grad_y(x: float, y: float) -> float:\n",
        "    \"\"\"Computes the partial derivative of f with respect to y.\n",
        "\n",
        "    Args:\n",
        "        x (float): The x-coordinate.\n",
        "        y (float): The y-coordinate.\n",
        "\n",
        "    Returns:\n",
        "        float: The gradient component df/dy.\n",
        "    \"\"\"\n",
        "    return 2 * (y - 2) + x\n",
        "\n",
        "def project_x(x: float) -> float:\n",
        "    \"\"\"Projects x onto the constraint set {x | x² ≥ 2}.\n",
        "\n",
        "    Args:\n",
        "        x (float): The x-coordinate before projection.\n",
        "\n",
        "    Returns:\n",
        "        float: The projected x-coordinate.\n",
        "    \"\"\"\n",
        "    return # Your code\n",
        "\n",
        "def ao_pgd(max_iters: int, tol: float, alpha_x: float, alpha_y: float) -> tuple[float, float, list[tuple[float, float]]]:\n",
        "    \"\"\"Performs alternating optimization projected gradient descent.\n",
        "\n",
        "    Args:\n",
        "        max_iters (int): Maximum number of iterations.\n",
        "        tol (float): Convergence tolerance.\n",
        "        alpha_x (float): Step size for x update.\n",
        "        alpha_y (float): Step size for y update.\n",
        "\n",
        "    Returns:\n",
        "        tuple[float, float, list[tuple[float, float]]]:\n",
        "            - The optimal x-coordinate.\n",
        "            - The optimal y-coordinate.\n",
        "            - A list of all (x, y) points visited during optimization.\n",
        "    \"\"\"\n",
        "    # Initialize with a feasible point (x² ≥ 2 and y ≥ 3)\n",
        "    x, y = -5.0, 4.0\n",
        "    # Store all iterations\n",
        "    history = [(x, y)]\n",
        "    print(f\"Iter 0: x = {x:.4f}, y = {y:.4f}, f(x,y) = {f(x, y):.4f}, Constraint: {(x**2 >= 2) and (y >= 3)}\")\n",
        "\n",
        "    for k in range(1, max_iters + 1):\n",
        "\n",
        "        # Step 1: Fix y, update x\n",
        "        x_temp = x - alpha_x * grad_x(x, y)\n",
        "\n",
        "        # Project onto x² ≥ 2\n",
        "        x_new = project_x(x_temp) if x_temp**2 < 2 else x_temp\n",
        "\n",
        "        # Step 2: Fix x_new, update y\n",
        "        y_temp = y - alpha_y * grad_y(x_new, y)\n",
        "\n",
        "        # Project onto y ≥ 3\n",
        "        y_new = # Your code\n",
        "\n",
        "        # Store the new point\n",
        "        history.append((x_new, y_new))\n",
        "\n",
        "        # Check convergence\n",
        "        if np.linalg.norm([x_new - x, y_new - y]) < tol:\n",
        "            x, y = x_new, y_new\n",
        "            print(f\"Iter {k}: x = {x:.4f}, y = {y:.4f}, f(x,y) = {f(x, y):.4f}, Constraint: {(x**2 >= 2) and (y >= 3)}\")\n",
        "            print(f\"Converged at iteration {k}\")\n",
        "            break\n",
        "\n",
        "        x, y = x_new, y_new\n",
        "        print(f\"Iter {k}: x = {x:.4f}, y = {y:.4f}, f(x,y) = {f(x, y):.4f}, Constraint: {(x**2 >= 2) and (y >= 3)}\")\n",
        "\n",
        "    return x, y, history\n",
        "\n",
        "# Run the algorithm\n",
        "x_opt, y_opt, history = ao_pgd(max_iters=1000, tol=1e-2, alpha_x=0.1, alpha_y=0.1)\n",
        "print(f\"\\nOptimal solution: x* = {x_opt:.4f}, y* = {y_opt:.4f}, Constraint satisfied: {(x_opt**2 >= 2) and (y_opt >= 3)}\")\n"
      ],
      "metadata": {
        "id": "u2cwBLQvKf5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create contour plot\n",
        "limit_low = -5\n",
        "limit_up = 5\n",
        "x_vals = np.linspace(limit_low, limit_up, 100)\n",
        "y_vals = np.linspace(limit_low, limit_up, 100)\n",
        "X, Y = np.meshgrid(x_vals, y_vals)\n",
        "Z = f(X, Y)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot contours\n",
        "contours = plt.contour(X, Y, Z, levels=20, cmap='viridis')\n",
        "plt.colorbar(contours, label='Objective Value')\n",
        "\n",
        "# Plot constraint boundaries\n",
        "# 1. For x² ≥ 2 (two vertical lines at x = ±√2)\n",
        "sqrt2 = np.sqrt(2)\n",
        "plt.axvline(x=sqrt2, color='r', linestyle='--', label=\"x = √2\")\n",
        "plt.axvline(x=-sqrt2, color='r', linestyle='--', label=\"x = -√2\")\n",
        "\n",
        "# 2. For y ≥ 3 (horizontal line)\n",
        "plt.axhline(y=3, color='b', linestyle='-.', label=\"y = 3\")\n",
        "\n",
        "# Shade feasible region (x² ≥ 2 AND y ≥ 3)\n",
        "# Left feasible region (x ≤ -√2 and y ≥ 3)\n",
        "x_left = np.linspace(limit_low, -sqrt2, 100)\n",
        "y_top = np.linspace(3, limit_up, 100)\n",
        "X_left, Y_top = np.meshgrid(x_left, y_top)\n",
        "plt.fill_between(x_left, 3, limit_up, color='red', alpha=0.1)\n",
        "\n",
        "# Right feasible region (x ≥ √2 and y ≥ 3)\n",
        "x_right = np.linspace(sqrt2, limit_up, 100)\n",
        "plt.fill_between(x_right, 3, limit_up, color='red', alpha=0.1, label='Feasible Region')\n",
        "\n",
        "# Plot PGD iterations\n",
        "x_hist, y_hist = zip(*history)\n",
        "plt.scatter(x_hist, y_hist, c='white', s=30, edgecolors='black', label='PGD Iterations')\n",
        "plt.plot(x_hist, y_hist, 'k--', lw=1, alpha=0.5)\n",
        "\n",
        "# Mark initial and optimal points\n",
        "plt.scatter(x_hist[0], y_hist[0], c='blue', s=100, label='Initial Point', marker='s')\n",
        "plt.scatter(x_opt, y_opt, c='red', s=100, label='Optimal Point', marker='*')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Projected Gradient Descent with x² ≥ 2 and y ≥ 3 Constraints')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xlim(limit_low, limit_up)\n",
        "plt.ylim(limit_low, limit_up)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "il7al33TLeM5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solve the above problem using simultanous PGD algorithm."
      ],
      "metadata": {
        "id": "q-t-NGSaxCq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x: float, y: float) -> float:\n",
        "    \"\"\"Computes the objective function value.\n",
        "\n",
        "    Args:\n",
        "        x (float): The x-coordinate.\n",
        "        y (float): The y-coordinate.\n",
        "\n",
        "    Returns:\n",
        "        float: The computed function value.\n",
        "    \"\"\"\n",
        "    return (x - 1)**2 + (y - 2)**2 + x * y\n",
        "\n",
        "def grad_x(x: float, y: float) -> float:\n",
        "    \"\"\"Computes the partial derivative of f with respect to x.\n",
        "\n",
        "    Args:\n",
        "        x (float): The x-coordinate.\n",
        "        y (float): The y-coordinate.\n",
        "\n",
        "    Returns:\n",
        "        float: The gradient component df/dx.\n",
        "    \"\"\"\n",
        "    return 2 * (x - 1) + y\n",
        "\n",
        "def grad_y(x: float, y: float) -> float:\n",
        "    \"\"\"Computes the partial derivative of f with respect to y.\n",
        "\n",
        "    Args:\n",
        "        x (float): The x-coordinate.\n",
        "        y (float): The y-coordinate.\n",
        "\n",
        "    Returns:\n",
        "        float: The gradient component df/dy.\n",
        "    \"\"\"\n",
        "    return 2 * (y - 2) + x\n",
        "\n",
        "def project_x(x: float) -> float:\n",
        "    \"\"\"Projects x onto the constraint set {x | x² ≥ 2}.\n",
        "\n",
        "    Args:\n",
        "        x (float): The x-coordinate before projection.\n",
        "\n",
        "    Returns:\n",
        "        float: The projected x-coordinate.\n",
        "    \"\"\"\n",
        "    return # Your code\n",
        "\n",
        "def simultaneous_pgd(max_iters: int, tol: float, alpha_x: float, alpha_y: float) -> tuple[float, float, list[tuple[float, float]]]:\n",
        "    \"\"\"Performs simultaneous projected gradient descent.\n",
        "\n",
        "    Args:\n",
        "        max_iters (int): Maximum number of iterations.\n",
        "        tol (float): Convergence tolerance.\n",
        "        alpha_x (float): Step size for x update.\n",
        "        alpha_y (float): Step size for y update.\n",
        "\n",
        "    Returns:\n",
        "        tuple[float, float, list[tuple[float, float]]]:\n",
        "            - The optimal x-coordinate.\n",
        "            - The optimal y-coordinate.\n",
        "            - A list of all (x, y) points visited during optimization.\n",
        "    \"\"\"\n",
        "    # Initialize with a feasible point (x² ≥ 2 and y ≥ 3)\n",
        "    x, y = -5.0, 4.0\n",
        "\n",
        "    # Store all iterations\n",
        "    history = [(x, y)]\n",
        "    print(f\"Iter 0: x = {x:.4f}, y = {y:.4f}, f(x,y) = {f(x, y):.4f}, Constraint: {(x**2 >= 2) and (y >= 3)}\")\n",
        "\n",
        "    for k in range(1, max_iters + 1):\n",
        "\n",
        "        # Compute updates simultaneously\n",
        "        x_temp = x - alpha_x * grad_x(x, y)\n",
        "        y_temp = y - alpha_y * grad_y(x, y)\n",
        "\n",
        "        # Project onto feasible set\n",
        "        x_new = project_x(x_temp) if x_temp**2 < 2 else x_temp\n",
        "\n",
        "        # Project onto y ≥ 3\n",
        "        y_new = # Your code\n",
        "\n",
        "        # Store the new point\n",
        "        history.append((x_new, y_new))\n",
        "\n",
        "        # Check convergence\n",
        "        if np.linalg.norm([x_new - x, y_new - y]) < tol:\n",
        "            print(f\"Converged at iteration {k}\")\n",
        "            break\n",
        "\n",
        "        x, y = x_new, y_new\n",
        "        print(f\"Iter {k}: x = {x:.4f}, y = {y:.4f}, f(x,y) = {f(x, y):.4f}, Constraint: {(x**2 >= 2) and (y >= 3)}\")\n",
        "\n",
        "    return x, y, history\n",
        "\n",
        "# Run the algorithm\n",
        "x_opt, y_opt, history = simultaneous_pgd(max_iters=1000, tol=1e-2, alpha_x=0.1, alpha_y=0.1)\n",
        "print(f\"\\nOptimal solution: x* = {x_opt:.4f}, y* = {y_opt:.4f}, Constraint satisfied: {(x_opt**2 >= 2) and (y_opt >= 3)}\")\n"
      ],
      "metadata": {
        "id": "qz3q9-iZM9Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create contour plot\n",
        "limit_low = -3\n",
        "limit_up = 6\n",
        "x_vals = np.linspace(limit_low, limit_up, 100)\n",
        "y_vals = np.linspace(limit_low, limit_up, 100)\n",
        "X, Y = np.meshgrid(x_vals, y_vals)\n",
        "Z = f(X, Y)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot contours\n",
        "contours = plt.contour(X, Y, Z, levels=20, cmap='viridis')\n",
        "plt.colorbar(contours, label='Objective Value')\n",
        "\n",
        "# Plot constraint boundaries\n",
        "# 1. For x² ≥ 2 (two vertical lines at x = ±√2)\n",
        "sqrt2 = np.sqrt(2)\n",
        "plt.axvline(x=sqrt2, color='r', linestyle='--', label=\"x = √2\")\n",
        "plt.axvline(x=-sqrt2, color='r', linestyle='--', label=\"x = -√2\")\n",
        "\n",
        "# 2. For y ≥ 3 (horizontal line)\n",
        "plt.axhline(y=3, color='b', linestyle='-.', label=\"y = 3\")\n",
        "\n",
        "# Shade feasible region (x² ≥ 2 AND y ≥ 3)\n",
        "# Left feasible region (x ≤ -√2 and y ≥ 3)\n",
        "x_left = np.linspace(limit_low, -sqrt2, 100)\n",
        "y_top = np.linspace(3, limit_up, 100)\n",
        "X_left, Y_top = np.meshgrid(x_left, y_top)\n",
        "plt.fill_between(x_left, 3, limit_up, color='red', alpha=0.1)\n",
        "\n",
        "# Right feasible region (x ≥ √2 and y ≥ 3)\n",
        "x_right = np.linspace(sqrt2, limit_up, 100)\n",
        "plt.fill_between(x_right, 3, limit_up, color='red', alpha=0.1, label='Feasible Region')\n",
        "\n",
        "# Plot PGD iterations\n",
        "x_hist, y_hist = zip(*history)\n",
        "plt.scatter(x_hist, y_hist, c='white', s=30, edgecolors='black', label='PGD Iterations')\n",
        "plt.plot(x_hist, y_hist, 'k--', lw=1, alpha=0.5)\n",
        "\n",
        "# Mark initial and optimal points\n",
        "plt.scatter(x_hist[0], y_hist[0], c='blue', s=100, label='Initial Point', marker='s')\n",
        "plt.scatter(x_opt, y_opt, c='red', s=100, label='Optimal Point', marker='*')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Projected Gradient Descent with x² ≥ 2 and y ≥ 3 Constraints')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xlim(limit_low, limit_up)\n",
        "plt.ylim(limit_low, limit_up)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LXD5pqjqNJ-J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}