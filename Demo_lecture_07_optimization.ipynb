{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "------------------\n",
        "```markdown\n",
        "# Copyright ¬© 2024 Meysam Goodarzi\n",
        "This notebook is licensed under CC BY-NC 4.0 with the following amandments:\n",
        "- Individuals may use, share, and adapt this material for non-commercial purposes with attribution.\n",
        "- Institutions/Companies must obtain written consent to use this material, except for nonprofits.\n",
        "- Commercial use is prohibited without permission.  \n",
        "Contact: analytica@meysam-goodarzi.com\n",
        "```\n",
        "------------------------------\n",
        "‚ùó‚ùó‚ùó **IMPORTANT**‚ùó‚ùó‚ùó **Create a copy of this notebook**\n",
        "\n",
        "In order to work with this Google Colab you need to create a copy of it. Please **DO NOT** provide your answers here. Instead, work on the copy version. To make a copy:\n",
        "\n",
        "**Click on: File -> save a copy in drive**\n",
        "\n",
        "Have you successfully created the copy? if yes, there must be a new tab opened in your browser. Now move to the copy and start from there!\n",
        "\n",
        "----------------------------------------------\n"
      ],
      "metadata": {
        "id": "TwkcaWdkKel3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from IPython.display import clear_output\n",
        "import plotly.graph_objects as go"
      ],
      "metadata": {
        "id": "jiCAAPJZIeTj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization\n",
        "This notebook is dedicated to the introduction to Gradient Descent, Gradient Steepest Descent, Newtons Method, and the Stochastic Gradient Descent."
      ],
      "metadata": {
        "id": "blOpvcCJIOmV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFW8GTe0RpTC"
      },
      "source": [
        "## Gradient Descent\n",
        "The general gradient descent update rule is\n",
        "$$\\theta := \\theta - \\alpha\\cdot\\nabla J(\\theta)$$\n",
        "where:\n",
        "* $\\theta$ is the parameter vector (the values over which you want to optimize your objective function)\n",
        "* $\\alpha$ is the learning rate\n",
        "* $\\nabla J(\\theta)$ is the gradient of the function $J(\\theta)$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example\n",
        "Let us consider the function $$f(x) = x^2 + 2x + 1$$ with the gradient of $$f^¬¥(x) = 2x + 2.$$ We can implement gradient descent algorithm as follows:"
      ],
      "metadata": {
        "id": "k6RTJAI5UuQ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqkB4uJYRpTD"
      },
      "outputs": [],
      "source": [
        "# Define the function and its gradient\n",
        "def f(x: float):\n",
        "    return x**2 + 2*x + 1\n",
        "\n",
        "def grad_f(x: float):\n",
        "    return 2*x + 2\n",
        "\n",
        "# Gradient Descent implementation\n",
        "def gradient_descent(starting_point: float, learning_rate: float, num_iterations: int):\n",
        "    x = starting_point\n",
        "    x_values = [x]\n",
        "    for i in range(num_iterations):\n",
        "        x = x - learning_rate * grad_f(x)\n",
        "        x_values.append(x)\n",
        "    return x_values\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.1\n",
        "num_iterations = 10\n",
        "starting_point = 5\n",
        "\n",
        "# Run gradient descent\n",
        "x_values = gradient_descent(starting_point, learning_rate, num_iterations)\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(7, 3))\n",
        "x_range = np.linspace(-5, 5, 100)\n",
        "ax.plot(x_range, f(x_range), label=\"f(x) = x^2\")\n",
        "ax.scatter(x_values, f(np.array(x_values)), color='red', label='GD Steps')\n",
        "ax.set(title=\"Gradient Descent on f(x) = x^2\", xlabel=\"x\", ylabel=\"f(x)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alZC7g0ARpTI"
      },
      "source": [
        "### Exercise 1\n",
        "Let us consider the function $$f(x, y) = x^2 + y^2 + 2x - 3y + 1 + 2.$$\n",
        "This could represent a policy scenario where there are two adjustable parameters, such as:\n",
        "* $x$: public spending on healthcare.\n",
        "* $y$: public spending on education.\n",
        "\n",
        "The function represents the total cost of these interventions, and we want to find the optimal combination that minimizes the overall cost. Write the following function to compelete the minimization using gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function with multiple local minima\n",
        "def f(x: float, y: float):\n",
        "    return x**2 + y**2 + 2*x - 3*y + 1 + 2\n",
        "\n",
        "# Gradient of the function (partial derivatives)\n",
        "def grad_f(x: float, y: float):\n",
        "    return np.array([2*x + 2, 2*y - 3])\n",
        "\n",
        "\n",
        "# Preparing the axes of the plot\n",
        "x_vals = np.linspace(-10, 10, 100)\n",
        "y_vals = np.linspace(-10, 10, 100)\n",
        "X, Y = np.meshgrid(x_vals, y_vals)\n",
        "Z = f(X, Y)\n",
        "\n",
        "steps_x = []\n",
        "steps_y = []\n",
        "steps_z = []\n",
        "\n",
        "# Parameters for the gradient descent\n",
        "learning_rate = 0.1\n",
        "num_iterations = 10\n",
        "starting_point = [8, 6]  # Try different starting points, e.g., [8, 6], [-5, 2], [3, -4]\n",
        "\n",
        "point = np.array(starting_point)\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Append current point\n",
        "    steps_x.append(point[0])\n",
        "    steps_y.append(point[1])\n",
        "    steps_z.append(f(point[0], point[1]))\n",
        "\n",
        "    # Create plotly figure\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add the 3D surface plot for the function\n",
        "    fig.add_trace(go.Surface(x=X, y=Y, z=Z, colorscale='Viridis', opacity=0.7))\n",
        "\n",
        "    # Add the steps taken by gradient descent\n",
        "    fig.add_trace(go.Scatter3d(x=steps_x, y=steps_y, z=steps_z,\n",
        "                                mode='markers+lines', marker=dict(size=5, color='red'),\n",
        "                                line=dict(color='red', width=3),\n",
        "                                name='Steps'))\n",
        "\n",
        "    # Customize plot details\n",
        "    fig.update_layout(title=f\"3D Gradient Descent\",\n",
        "                      scene=dict(xaxis_title=\"x\", yaxis_title=\"y\", zaxis_title=\"f(x, y)\"),\n",
        "                      showlegend=True)\n",
        "    fig.show()\n",
        "\n",
        "    # Update point using gradient descent rule\n",
        "    point = point - learning_rate * grad_f(point[0], point[1])"
      ],
      "metadata": {
        "id": "OPw-_232bIhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neccXI6IRpTL"
      },
      "source": [
        "**Question**: What whappens if gradient descent is applied on a function with multiple minima/maxima?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keGnGxfoRpTL"
      },
      "source": [
        "### Exercise 2\n",
        "Let us consider the function $$f(x, y) = x^4 - 4x^2 + 2.$$\n",
        "This function has multiple minimas. Write a gradient descent algorithm to find the minimas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYRjad6dRpTL"
      },
      "outputs": [],
      "source": [
        "# Define the function and its gradient\n",
        "def f(x: float):\n",
        "    return x**4 - 4*x**2 + 2\n",
        "\n",
        "def grad_f(x: float):\n",
        "    return 4*x**3 - 8*x\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(7, 3))\n",
        "x_range = np.linspace(-2, 2, 100)\n",
        "ax.plot(x_range, f(x_range), label=\"f(x) = x^4 -4x + 2 \")\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.02\n",
        "num_iterations = 20\n",
        "starting_point = 0.6\n",
        "\n",
        "# Gradient Descent implementation\n",
        "x = starting_point\n",
        "x_values = [x]\n",
        "for i in range(num_iterations):\n",
        "    x = # Your code\n",
        "    x_values.append(x)\n",
        "    ax.scatter(x_values, f(np.array(x_values)), color='red')\n",
        "\n",
        "ax.set(title=\"Gradient Descent\", xlabel=\"x\", ylabel=\"f(x)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: What happens if change the minus sign to plus in the gradient descent update formula?"
      ],
      "metadata": {
        "id": "SQ9xpjDWoV-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x: float):\n",
        "    return x**4 - 4*x**2 + 2\n",
        "\n",
        "def grad_f(x: float):\n",
        "    return 4*x**3 - 8*x\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(7, 3))\n",
        "x_range = np.linspace(-2, 2, 100)\n",
        "ax.plot(x_range, f(x_range), label=\"f(x) = x^4 -4x + 2 \")\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.02\n",
        "num_iterations = 20\n",
        "starting_point = 0.6\n",
        "\n",
        "# Gradient Descent implementation\n",
        "x = starting_point\n",
        "x_values = [x]\n",
        "for i in range(num_iterations):\n",
        "    x = # Your code\n",
        "    x_values.append(x)\n",
        "    ax.scatter(x_values, f(np.array(x_values)), color='red')\n",
        "\n",
        "ax.set(title=\"Gradient Descent on f(x) = x^2\", xlabel=\"x\", ylabel=\"f(x)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bw2IHYQBotx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Newton's Method\n",
        "Newton‚Äôs Method uses second-order information ([Hessian](https://en.wikipedia.org/wiki/Hessian_matrix#:~:text=In%20mathematics%2C%20the%20Hessian%20matrix,a%20function%20of%20many%20variables.) matrix) to find optimal steps. It converges faster than gradient descent but is computationally more expensive. The general update rule is\n",
        "$$\\theta := \\theta - H^{-1}\\cdot\\nabla J(\\theta)$$\n",
        "where:\n",
        "* $\\theta$ is the parameter vector (the values over which you want to optimize your objective function)\n",
        "* $\\nabla J(\\theta)$ is the gradient of the function $J(\\theta),$ and\n",
        "* $H$ denotes the Hessian matrix.\n",
        "\n",
        "<details>\n",
        "  <summary>Mathematical explanations</summary>\n",
        "</p>\n",
        "\n",
        "Newton‚Äôs method is derived from the second-order Taylor expansion of the function $J(\\theta)$ around $\\theta_k$:\n",
        "\n",
        "$$\n",
        "J(\\theta) \\approx J(\\theta_k) + \\nabla J(\\theta_k)^T (\\theta - \\theta_k) + \\frac{1}{2} (\\theta - \\theta_k)^T H (\\theta - \\theta_k).\n",
        "$$\n",
        "\n",
        "To find the optimal update step, we set the derivative of the Taylor approximation to zero:\n",
        "\n",
        "$$\n",
        "\\nabla J(\\theta) \\approx \\nabla J(\\theta_k) + H (\\theta - \\theta_k) = 0.\n",
        "$$\n",
        "\n",
        "Solving for $\\theta$, we get:\n",
        "\n",
        "$$\n",
        "\\theta_{k+1} = \\theta_k - H^{-1} \\nabla J(\\theta_k).\n",
        "$$\n",
        "\n",
        "This forms the iterative update rule for Newton‚Äôs method:\n",
        "\n",
        "$$\n",
        "\\theta_{k+1} = \\theta_k - H^{-1} \\nabla J(\\theta_k).\n",
        "$$\n",
        "</p>\n",
        "</details>"
      ],
      "metadata": {
        "id": "MG4umONxR1X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example\n",
        "Let us implement the method on a simple function given by\n",
        "$$f(x) = (x-3)^2$$"
      ],
      "metadata": {
        "id": "UhrmPKvjSxkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def newtons_method(f_grad, hess, x_init, tol=1e-6, max_iter=100):\n",
        "    x = x_init\n",
        "    for _ in range(max_iter):\n",
        "        grad = f_grad(x)\n",
        "        if np.linalg.norm(grad) < tol:\n",
        "            break\n",
        "        x = x - 1/hess * grad\n",
        "    return x\n",
        "\n",
        "# Example: f(x) = (x - 3)^2\n",
        "f_grad = lambda x: 2 * (x - 3)\n",
        "f_hess = 2\n",
        "\n",
        "x_opt = newtons_method(f_grad, f_hess, x_init=np.array([10.0]))\n",
        "print(\"Optimized x:\", x_opt)"
      ],
      "metadata": {
        "id": "hJ1RUBEKR31B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3"
      ],
      "metadata": {
        "id": "wA62a9ErBUSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the function:\n",
        "$$f(x, y) = e^{x + y} + (x - 2)^2 + (y + 3)^2.$$ Find the minimum of the function using Newton's method.\n",
        "<details>\n",
        "<summary>\n",
        "Hint\n",
        "</summary>\n",
        "<p>\n",
        "The gradient of $f(x, y)$ is given by:\n",
        "$$\n",
        "    \\nabla f(x, y) =\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial f}{\\partial x} \\\\\n",
        "\\frac{\\partial f}{\\partial y}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "e^{x+y} + 2(x - 2) \\\\\n",
        "e^{x+y} + 2(y + 3)\n",
        "\\end{bmatrix}\n",
        ".$$\n",
        "\n",
        "The Hessian matrix is:\n",
        "$$\n",
        "H(x, y) =\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\\n",
        "\\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "e^{x+y} + 2 & e^{x+y} \\\\\n",
        "e^{x+y} & e^{x+y} + 2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "</p>\n",
        "</details>"
      ],
      "metadata": {
        "id": "7QbbqaqPBcka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def newtons_method(f_grad, f_hess, x_init, tol=1e-6, max_iter=100):\n",
        "    x = x_init\n",
        "    for i in range(max_iter):\n",
        "        grad = f_grad(x)  # Gradient vector\n",
        "        # Hessian matrix\n",
        "        hess = # Your code\n",
        "        if np.linalg.norm(grad) < tol:\n",
        "            print(f\"Converged in {i+1} iterations\")\n",
        "            break\n",
        "        x = x - np.linalg.inv(hess) @ grad  # Newton's update step\n",
        "        print(x)\n",
        "    return x\n",
        "\n",
        "# Function: f(x, y) = e^(x+y) + (x - 2)^2 + (y + 3)^2\n",
        "f_grad = lambda x: np.array([\n",
        "    np.exp(x[0] + x[1]) + 2 * (x[0] - 2),\n",
        "    np.exp(x[0] + x[1]) + 2 * (x[1] + 3)\n",
        "])\n",
        "\n",
        "f_hess = lambda x: np.array([\n",
        "    # Your code\n",
        "])\n",
        "\n",
        "x_opt = newtons_method(f_grad, f_hess, x_init=np.array([10.0, -15.0]))\n",
        "print(\"Optimized x:\", x_opt)"
      ],
      "metadata": {
        "id": "9QKzKK9oBScW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stochastic Gradient Descent (SGD)\n",
        "SGD is used for large datasets where computing the full gradient is expensive. It updates parameters using randomly selected data points. The general idea is to compute the gradient on bath of data instead of the whole function. That is:\n",
        "$$\\nabla f(x) \\approx \\frac{1}{B}\\sum_i^{B}\\nabla f_i(x).$$"
      ],
      "metadata": {
        "id": "OO9p4tkWTKDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example\n",
        "Let us generate some dummy data using the following relationship between $x$ and $y$ and addition of some random values.\n",
        "$$y = 2x + 1 + \\text{random value}.$$\n",
        "\n",
        "Assuming, absurdly, that we are solving a linear regression problem $$y = wx + b$$ and we do not know the coefficient $w$ and the intercept $b$. The goal is to use SGD to obtain the coefficient and the intercept."
      ],
      "metadata": {
        "id": "9WaMJsrYbOkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Generate dummy dataset (y = 2x + 1 with noise)\n",
        "random.seed(42)\n",
        "X = np.array([random.uniform(-10, 10) for _ in range(256)])\n",
        "y = np.array([2 * x + 1 + random.uniform(-1, 1) for x in X])\n",
        "\n",
        "def sgd(X, y, alpha=0.01, epochs=100, batch_size=16):\n",
        "    w, b = 0, 0\n",
        "    n = len(X)\n",
        "    losses = []\n",
        "    for epoch in range(epochs):\n",
        "        grad_w, grad_b = 0, 0\n",
        "        indices = np.random.permutation(n)\n",
        "        X, y = X[indices], y[indices]\n",
        "\n",
        "        for i in range(0, n, batch_size):\n",
        "            xi = X[i:i+batch_size]\n",
        "            yi = y[i:i+batch_size]\n",
        "            grad_w = 1/batch_size*np.sum(-2 * (yi - (w * xi + b)) * xi)\n",
        "            grad_b = 1/batch_size*np.sum(-2 * (yi - (w * xi + b)))\n",
        "\n",
        "            # Update parameters using the average gradient\n",
        "            w -= alpha * grad_w\n",
        "            b -= alpha * grad_b\n",
        "        epoch_loss = 1/n*np.sum((y-(w*X+b))**2)\n",
        "        losses.append(float(epoch_loss.round(2)))\n",
        "\n",
        "    return w, b, losses\n",
        "\n",
        "num_epochs = 100\n",
        "w_batch, b_batch, losses = sgd(X, y, epochs=num_epochs)\n",
        "print(f\"Batch SGD Parameters: w = {w_batch:.2f}, b = {b_batch:.2f}\")\n",
        "print(f\"Epoch Losses: {losses}\")\n",
        "# Plot loss over epochs\n",
        "plt.plot(range(num_epochs), losses)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss Over Time Using SGD\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "izC4DBJbTUop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4\n",
        "Predict students' exam scores based on their study hours using a randomly generated dataset given below.\n",
        "\n",
        "<details>\n",
        "<summary>\n",
        "Hint\n",
        "</summary>\n",
        "<p>\n",
        "The exam score data is generated using this formula:\n",
        "$$y = 5x + 50 + noise$$\n",
        "where $y$ is the score, $x$ is the number of study hours, and $noise$ is added to make the data random.\n",
        "</p>\n",
        "</details>"
      ],
      "metadata": {
        "id": "0f2tHS5SI4CH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate random study hours (between 0 to 12 hours)\n",
        "np.random.seed(42)\n",
        "X = np.random.uniform(0, 12, 100)\n",
        "\n",
        "# Generate exam scores using y = 5 * x + 50 + noise\n",
        "true_w, true_b = 5, 50  # True relationship\n",
        "noise = np.random.normal(0, 5, size=X.shape)  # Add some noise\n",
        "y = true_w * X + true_b + noise\n",
        "\n",
        "# Normalize X for better convergence\n",
        "X = (X - np.mean(X)) / np.std(X)\n",
        "\n",
        "def sgd(X, y, alpha=0.01, epochs=100, batch_size=10):\n",
        "    w, b = 0, 0  # Initialize weights and bias\n",
        "    n = len(X)\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        indices = np.random.permutation(n)\n",
        "        X, y = X[indices], y[indices]  # Shuffle data\n",
        "\n",
        "        for i in range(0, n, batch_size):\n",
        "            xi = X[i:i+batch_size]\n",
        "            yi = y[i:i+batch_size]\n",
        "\n",
        "            # Compute gradients\n",
        "            grad_w = -2/batch_size * np.sum((yi - (w * xi + b)) * xi)\n",
        "            grad_b = # Your code\n",
        "\n",
        "            # Update weights\n",
        "            w -= # Your code\n",
        "            b -= alpha * grad_b\n",
        "\n",
        "        # Compute loss\n",
        "        epoch_loss = # Your code\n",
        "        losses.append(epoch_loss)\n",
        "\n",
        "    return w, b, losses\n",
        "\n",
        "# Train the model using SGD\n",
        "num_epochs = 100\n",
        "w_sgd, b_sgd, losses = sgd(X, y, epochs=num_epochs)\n",
        "\n",
        "# Predictions using learned parameters\n",
        "X_test = np.linspace(min(X), max(X), 100)\n",
        "y_pred = w_sgd * X_test + b_sgd\n",
        "\n",
        "# Plot loss over epochs\n",
        "plt.plot(range(num_epochs), losses)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss Over Time Using SGD\")\n",
        "plt.show()\n",
        "\n",
        "# Plot the data and model prediction\n",
        "plt.scatter(X, y, label=\"Actual Data\", alpha=0.6)\n",
        "plt.plot(X_test, y_pred, color=\"red\", label=\"SGD Prediction\")\n",
        "plt.xlabel(\"Normalized Study Hours\")\n",
        "plt.ylabel(\"Exam Score\")\n",
        "plt.legend()\n",
        "plt.title(\"Study Hours vs Exam Score Prediction using SGD\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Estimated Parameters: w = {w_sgd:.2f}, b = {b_sgd:.2f}\")\n"
      ],
      "metadata": {
        "id": "ObMHDsrxI3zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Congratulations! You have finished the Notebook! Great Job!**\n",
        "ü§óüôåüëçüëèüí™\n",
        "<!--\n",
        "# Copyright ¬© 2024 Meysam Goodarzi\n",
        "This notebook is licensed under CC BY-NC 4.0 with the following amandments:\n",
        "- Individuals may use, share, and adapt this material for non-commercial purposes with attribution.\n",
        "- Institutions/Companies must obtain written consent to use this material, except for nonprofits.\n",
        "- Commercial use is prohibited without permission.  \n",
        "Contact: analytica@meysam-goodarzi.com.\n",
        "-->"
      ],
      "metadata": {
        "id": "hm7Ti41XJwm6"
      }
    }
  ]
}